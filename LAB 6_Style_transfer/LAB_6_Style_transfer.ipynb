{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzbD1yMxNSKA"
      },
      "source": [
        "#LAB 6: Style transfer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sho04lzVNSKG"
      },
      "source": [
        "\n",
        "\n",
        "# Neural Stylization with Pytorch\n",
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31IptKKUNSKG"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1crqAv5aoJ3uMSDFT99l6VuzY3VUlF_C7\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
        "\n",
        "\n",
        "This lab introduces a Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm takes two images, a content-image, and a style-image, and generates the optimized image to resemble the content of the content-image and the artistic style of the style-image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXA8tEZBNSKH"
      },
      "source": [
        "**Main idea**\n",
        "\n",
        "To transfer the style image $\\textit{I}_s$ onto a content image $\\textit{I}_c$ we synthesize a new image that simultaneously matches the content representation of $\\textit{I}_c$ and the style representation of $\\textit{I}_s$ (Fig). Thus we jointly minimize the distance of the feature representations of a white noise image from the content representation of the photograph in one layer and the style representation of the painting defined on a number of layers of the Convolutional Neural Network.\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1BhfqqQEMn3AyHpc6fB-shQDlU-tQeoLz\" alt=\"no_image\" style=\"width: 900px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrXzD4ieNSKI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmudnoptNSKJ"
      },
      "source": [
        "**Style and content image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkkR8qT0NSKK"
      },
      "outputs": [],
      "source": [
        "style_img_name = \"lab6/style/mosaic.jpg\"\n",
        "content_img_name = \"lab6/content/Tuebingen_Neckarfront.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpiN1kl6NSKM"
      },
      "outputs": [],
      "source": [
        "style_img = Image.open(style_img_name)\n",
        "content_img = Image.open(content_img_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnpidbK3NSKO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('style image')\n",
        "plt.imshow(style_img)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('content image')\n",
        "plt.imshow(content_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMw6mygtNSKP"
      },
      "source": [
        "### Style representation\n",
        "\n",
        "To obtain a representation of the style of an input image, we use a feature space designed to capture texture information. This feature space can be built on top of the filter responses in any layer of the network. It consists of the correlations between the different filter responses, where the expectation is taken over the spatial extent of the feature maps. These feature correlations are given by the Gram matrix $\\mathit{G} \\in \\mathcal{R}^{\\mathit{N_l}  \\times  \\mathit{N_l}}$, where $\\mathit{G^l_{ij}}$ is the inner product between the vectorised feature maps $i$ and $j$ in layer $l$:\n",
        "\n",
        "### <center> ${\\mathit{G^l_{ij}} = \\sum_{k}{F^l_{ik}F^l_{jk}}}$ </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41r9UTI7NSKQ"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(y):\n",
        "    (b, ch, h, w) = y.size()\n",
        "    features = y.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (ch * h * w)\n",
        "    return gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKk8vbNNSKS"
      },
      "source": [
        "### Deep image representations\n",
        "\n",
        "In this work we show how the generic feature representations learned by high-performing Convolutional Neural Networks can be used to independently process and manipulate the content and the style of natural images.\n",
        "\n",
        "The image reperesentations were generated on the basis of the VGG network, which was trained to perform object recognition and localization. We use the feature space provided by the 16 convolutional and 5 pooling layers of the 19-layer VGG network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMZXLeQYNSKT"
      },
      "outputs": [],
      "source": [
        "import torchvision.models.vgg as vgg\n",
        "\n",
        "class LossNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LossNetwork, self).__init__()\n",
        "        # get vgg network\n",
        "        self.vgg_layers = vgg.vgg19(pretrained=True).features\n",
        "\n",
        "\n",
        "    def forward(self, x, layer_name):\n",
        "        output = {}\n",
        "        for name, module in self.vgg_layers._modules.items():\n",
        "            x = module(x)\n",
        "            if name in layer_name:\n",
        "                output[layer_name[name]] = x\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSg3V-C-NSKW"
      },
      "outputs": [],
      "source": [
        "loss_net = LossNetwork().cuda()\n",
        "for param in loss_net.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qe2KbLENSKZ"
      },
      "source": [
        "Content representation is on layer ‘conv4 2’ and the style representation is on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ueqv4e1NSKZ"
      },
      "outputs": [],
      "source": [
        "style_layer_name = {\n",
        "    '1': \"conv1-1\",\n",
        "    '6': \"conv2-1\",\n",
        "    '11': \"conv3-1\",\n",
        "    '20': \"conv4-1\",\n",
        "    '29': \"conv5-1\"\n",
        "}\n",
        "\n",
        "content_layer_name = {\n",
        "    '22': \"conv4-2\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub7v8fWyNSKb"
      },
      "source": [
        "**Pre and post processing for images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2LcPEr7NSKb"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_size = 512\n",
        "prep = transforms.Compose([transforms.Resize(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
        "                        std=[1,1,1]),\n",
        "])\n",
        "\n",
        "post = transforms.Compose([\n",
        "    transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
        "                        std=[1,1,1]),\n",
        "    transforms.Lambda(lambda x: torch.clamp(x,0,1)),\n",
        "    transforms.ToPILImage()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx5rgh0NNSKd"
      },
      "outputs": [],
      "source": [
        "content = prep(content_img).cuda().unsqueeze(0)\n",
        "style = prep(style_img).cuda().unsqueeze(0)\n",
        "\n",
        "out_img = content.clone()\n",
        "out_img.requires_grad = True\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "style_gt = [gram_matrix(f).detach() for f in loss_net(style, style_layer_name).values()]\n",
        "content_gt = [A.detach() for A in loss_net(content, content_layer_name).values()]\n",
        "\n",
        "show_iter = 50\n",
        "optimizer = optim.LBFGS([out_img])\n",
        "n_iter = [0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-z1mhtANSKf"
      },
      "source": [
        "**Hyper parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0vjneX8NSKf"
      },
      "outputs": [],
      "source": [
        "alpha = 1e0\n",
        "beta = 1e7\n",
        "max_iter = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fSdtUsaNSKh"
      },
      "source": [
        "**Optimizing image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqeU7ZAsNSKi"
      },
      "outputs": [],
      "source": [
        "while n_iter[0] <= max_iter:\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        style_layers = [gram_matrix(f) for f in loss_net(out_img, style_layer_name).values()]\n",
        "        content_layers = [f for f in loss_net(out_img, content_layer_name).values()]\n",
        "\n",
        "        style_loss = 0\n",
        "        for i in range(len(style_layers)):\n",
        "            style_loss += criterion(style_layers[i], style_gt[i])\n",
        "\n",
        "        content_loss = criterion(content_layers[0], content_gt[0])\n",
        "\n",
        "        loss = alpha * content_loss + beta * style_loss\n",
        "        loss.backward()\n",
        "        n_iter[0]+=1\n",
        "        #print loss\n",
        "        if n_iter[0]%show_iter == (show_iter-1):\n",
        "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "#display result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzP5tE3NSKj"
      },
      "source": [
        "**Visualize result images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI-WwZpLNSKk"
      },
      "outputs": [],
      "source": [
        "out_img_hr = post(out_img.data[0].cpu().squeeze())\n",
        "\n",
        "plt.figure(figsize = (15,15))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('original image')\n",
        "plt.imshow(content_img)\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('style image')\n",
        "plt.imshow(style_img)\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('transfered image')\n",
        "plt.imshow(out_img_hr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boZSDoBzNSKm"
      },
      "source": [
        "\n",
        "\n",
        "# Perceptual Losses for Style Transfer with Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptzuxx33NSKm"
      },
      "source": [
        "**Main idea**\n",
        "\n",
        "Previous method produces high-quality results, but is computationally expensive since each step of the optimization problem requires a forward and backward pass through the pretrained network. To overcome this computational burden, we train a feed-forward network to quickly approximate solutions to their optimization problem.\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1GFM9l-63SsOFYHP-g2WNN2T6zNDdD09Z\" alt=\"no_image\" style=\"width: 900px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUnA47gNSKn"
      },
      "source": [
        "$\\mathcal{L}_{total} = \\alpha\\sum_{}{\\mathcal{l}_{content}} + \\beta\\sum_{}{\\mathcal{l}_{style}} + \\gamma\\mathcal{l}_{TV}$\n",
        "\n",
        "- $\\mathcal{l}_{content}^{\\phi,j} = ||\\phi_j(\\hat{y}) - \\phi_j(y)||^2_2$, $\\phi$ represents vgg feature\n",
        "\n",
        "- $\\mathcal{l}_{style}^{\\phi,j} = ||G^{\\phi}_{j}(\\hat{y}) - G^{\\phi}_{j}(y)||^2_F$, $G$ represents gram matrix.\n",
        "\n",
        "- $\\mathcal{l}_{TV}(y) = \\sum_{i,j}{|y_{i+1,j}-y_{i,j}|+|y_{i,j+1}-y_{i,j}|}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_PykzFzNSKo"
      },
      "source": [
        "**Data loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h27pG0nyNSKp"
      },
      "outputs": [],
      "source": [
        "data_root = \"lab6/train\"\n",
        "image_size = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(data_root, transform)\n",
        "\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZyYa10nNSKs"
      },
      "source": [
        "### 1.1 Write code (Image Transform Net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH7GdWNHNSKt"
      },
      "source": [
        "<img src=\"http://drive.google.com/uc?export=view&id=1OQzOwfRgpncvvBCwUCJbShYQGBElUqjW\" alt=\"no_image\" style=\"width: 400px;\"/>\n",
        "\n",
        "##### Residual block (channel, x)\n",
        "- Conv 1: $3\\times3$ Conv(in: channel, out: channel, padding: 1) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- Conv 2: $3\\times3$ Conv(in: channel, out: channel, padding: 1) with reflection padding\n",
        "- Instance Norm\n",
        "- Residual Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVAC52-zNSKt"
      },
      "source": [
        "#### ImageTransformNet\n",
        "- Conv $9\\times9$  (in: 3, out: 32, padding: 4) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- Conv $3\\times3$  (in: 32, out: 64, padding: 1, **stride: 2**) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- Conv $3\\times3$  (in: 64, out:128, padding: 1, **stride: 2**) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- 5 Residual block(channel: 128)\n",
        "- $2\\times$ Nearest Upsample\n",
        "- Conv $3\\times3$  (in: 128, out:64, padding: 1) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- **$2\\times$ Nearest Upsample**\n",
        "- Conv $3\\times3$  (in: 64, out:32, padding: 1) with reflection padding\n",
        "- Instance Norm\n",
        "- Relu\n",
        "- Conv $9\\times9$  (in: 32, out:3, padding: 4) with reflection padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2xohkIANSKu"
      },
      "outputs": [],
      "source": [
        "class ImageTransformNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageTransformNet, self).__init__()\n",
        "        #############\n",
        "        # CODE HERE #\n",
        "        #############\n",
        "\n",
        "    def forward(self, X):\n",
        "        #############\n",
        "        # CODE HERE #\n",
        "        #############\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        if upsample:\n",
        "            self.upsample_layer = torch.nn.Upsample(\n",
        "                mode='nearest', scale_factor=upsample)\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = self.upsample_layer(x_in)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENX1rkAoNSKv"
      },
      "outputs": [],
      "source": [
        "transformer = ImageTransformNet().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UECjvOs_NSK0"
      },
      "source": [
        "### 1.2 Find style and content representation layers in loss net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOhy07GgNSK0"
      },
      "outputs": [],
      "source": [
        "style_layer_name = {\n",
        "   #############\n",
        "    # CODE HERE #\n",
        "    #############\n",
        "}\n",
        "\n",
        "content_layer_name = {\n",
        "    #############\n",
        "    # CODE HERE #\n",
        "    #############\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDHvTvaRNSK3"
      },
      "outputs": [],
      "source": [
        "style = transform(style_img).cuda().unsqueeze(0)\n",
        "style_gt = [gram_matrix(f).detach() for f in loss_net(style, style_layer_name).values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXi16BWwNSK4"
      },
      "source": [
        "**Hyper parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR2MmZeaNSK5"
      },
      "outputs": [],
      "source": [
        "# You may adjust hyper paramters\n",
        "alpha = 1e0\n",
        "beta = 1e4\n",
        "gamma = 1e-5\n",
        "\n",
        "LR = 1e-3\n",
        "steps = 2000\n",
        "optimizer = optim.Adam(transformer.parameters(), LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hTIzqhNSK6"
      },
      "source": [
        "### 1.3 Train the image transformNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z524K6QKNSK7"
      },
      "source": [
        "- Print total loss, content loss, style loss and total variation loss for every 50 iterations\n",
        "- Style loss should contain all of the layers listed above\n",
        "- Use alpha, beta, gamma as coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jLdaI0ANSK7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "transformer.train()\n",
        "n_iter = 0\n",
        "class Found(Exception): pass\n",
        "try:\n",
        "    while True:\n",
        "        for x, _ in train_loader:\n",
        "\n",
        "            n_iter += 1\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.cuda()\n",
        "            # Image Transform\n",
        "            y = transformer(x)\n",
        "\n",
        "\n",
        "            #############\n",
        "            # CODE HERE #\n",
        "            #############\n",
        "\n",
        "            # Content Loss\n",
        "            content_x =\n",
        "            content_y =\n",
        "\n",
        "            content_loss =\n",
        "\n",
        "            # Style Loss\n",
        "            style_y =\n",
        "\n",
        "            style_loss = 0.\n",
        "            for i in range(len(style_y)):\n",
        "                style_loss +=\n",
        "\n",
        "            # Total Variation Loss\n",
        "            tv_loss =  gamma * (\n",
        "                torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) +\n",
        "                torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])))\n",
        "\n",
        "\n",
        "            total_loss = content_loss + style_loss + tv_loss\n",
        "            total_loss.backward()\n",
        "\n",
        "            total_running_loss += total_loss.item()\n",
        "            style_running_loss += style_loss.item()\n",
        "            content_running_loss += content_loss.item()\n",
        "            tv_running_loss += tv_loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if n_iter % show_iter == 0:\n",
        "                print('Iteration: %d, total loss: %.2f, content loss: %.2f, style loss: %.2f tv loss: %.2f'\n",
        "                      %(n_iter, total_running_loss/show_iter, content_running_loss/show_iter,\n",
        "                        style_running_loss/show_iter, tv_running_loss/show_iter))\n",
        "                total_running_loss = 0.0\n",
        "                style_running_loss = 0.0\n",
        "                content_running_loss = 0.0\n",
        "                tv_running_loss = 0.0\n",
        "\n",
        "            if n_iter >= steps:\n",
        "                raise Found\n",
        "\n",
        "except Found:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WmlI4EFNSK9"
      },
      "source": [
        "### 1.4 Discuss the result\n",
        "- Compare the result of neural style with yours\n",
        "- Adjust the hyper parameter and analyze each result\n",
        "- Use transform function before and after inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbovZhxhNSK9"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "                         std=[1/0.229, 1/0.224, 1/0.255]),\n",
        "    transforms.Lambda(lambda x: torch.clamp(x,0,1)),\n",
        "    transforms.ToPILImage()\n",
        "])\n",
        "\n",
        "#############\n",
        "# CODE HERE #\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kn_Kh1yNSK_"
      },
      "source": [
        "### *References*\n",
        "[1] Neural Transfer Pytorch Tutorial (https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}